Each map task in Hadoop is broken into the following phases: record reader, mapper, combiner, and partitioner.
The output of the map tasks, called the intermediate keys and values, are sent to the reducers. The reduce tasks are broken
into the following phases: shuffle, sort, reducer, and output format.

Pig and Hive are higher-level abstractions of MapReduce. They provide an interface that has nothing to do with "map" or "reduce", but
the systems interpret th higher-level language into a series of MapReduce jobs. They translate their respective languages into MR operations.

1. The patterns in this chapter are numerical summarizations, inverted index, and counting
with counters. They are more straightforward applications of MapReduce than some of
the other patterns in this book.
1.1 Numerical Summarizations
	calculating aggregate statistical values over your data.
Group records together by a key field and calculate a numerical aggretagete per group to get a top-level view of the larger data set.
min, max, avarage, median, and standard deviation.

1. InputFormat
 InputFormat describes the input-specification for a Map-Reduce job.

The Map-Reduce framework relies on the InputFormat of the job to:

    Validate the input-specification of the job.
    Split-up the input file(s) into logical InputSplits, each of which is then assigned to an individual Mapper.
    Provide the RecordReader implementation to be used to glean input records from the logical InputSplit for processing by the Mapper.

The default behavior of file-based InputFormats, typically sub-classes of FileInputFormat, is to split the input into logical InputSplits based on the total size, in bytes, of the input files. However, the FileSystem blocksize of the input files is treated as an upper bound for input splits. A lower bound on the split size can be set via mapreduce.input.fileinputformat.split.minsize.

Clearly, logical splits based on input-size is insufficient for many applications since record boundaries are to respected. In such cases, the application has to also implement a RecordReader on whom lies the responsibilty to respect record-boundaries and present a record-oriented view of the logical InputSplit to the individual task. 

2. InputSplit
InputSplit represents the data to be processed by an individual Mapper.
Typically, it presents a byte-oriented view on the input and is the responsibility of RecordReader of the job to process this and present a record-oriented view. 

3. RecordReader
 RecordReader reads <key, value> pairs from an InputSplit.

RecordReader, typically, converts the byte-oriented view of the input, provided by the InputSplit, and presents a record-oriented view for the Mapper & Reducer tasks for processing. It thus assumes the responsibility of processing record boundaries and presenting the tasks with keys and values.

4. OutputFormat

OutputFormat describes the output-specification for a Map-Reduce job.

The Map-Reduce framework relies on the OutputFormat of the job to:

*    Validate the output-specification of the job. For e.g. check that the output directory doesn't already exist.
*    Provide the RecordWriter implementation to be used to write out the output files of the job. Output files are stored in a FileSystem.

5. RecordWriter
 RecordWriter writes the output <key, value> pairs to an output file.

RecordWriter implementations write the job outputs to the FileSystem. 

6. Mapper
 Maps input key/value pairs to a set of intermediate key/value pairs.

Maps are the individual tasks which transform input records into a intermediate records. The transformed intermediate records need not be of the same type as the input records. A given input pair may map to zero or many output pairs.

The Hadoop Map-Reduce framework spawns one map task for each InputSplit generated by the InputFormat for the job. Mapper implementations can access the JobConf for the job via the JobConfigurable.configure(JobConf) and initialize themselves. Similarly they can use the Closeable.close() method for de-initialization.

The framework then calls map(Object, Object, OutputCollector, Reporter) for each key/value pair in the InputSplit for that task.

All intermediate values associated with a given output key are subsequently grouped by the framework, and passed to a Reducer to determine the final output. Users can control the grouping by specifying a Comparator via JobConf.setOutputKeyComparatorClass(Class).

The grouped Mapper outputs are partitioned per Reducer. Users can control which keys (and hence records) go to which Reducer by implementing a custom Partitioner.

Users can optionally specify a combiner, via JobConf.setCombinerClass(Class), to perform local aggregation of the intermediate outputs, which helps to cut down the amount of data transferred from the Mapper to the Reducer.

The intermediate, grouped outputs are always stored in SequenceFiles. Applications can specify if and how the intermediate outputs are to be compressed and which CompressionCodecs are to be used via the JobConf.

If the job has zero reduces then the output of the Mapper is directly written to the FileSystem without grouping by keys.

6. Reducer

 Reduces a set of intermediate values which share a key to a smaller set of values.

The number of Reducers for the job is set by the user via JobConf.setNumReduceTasks(int). Reducer implementations can access the JobConf for the job via the JobConfigurable.configure(JobConf) method and initialize themselves. Similarly they can use the Closeable.close() method for de-initialization.

Reducer has 3 primary phases:

    Shuffle

    Reducer is input the grouped output of a Mapper. In the phase the framework, for each Reducer, fetches the relevant partition of the output of all the Mappers, via HTTP.
    Sort

    The framework groups Reducer inputs by keys (since different Mappers may have output the same key) in this stage.

    The shuffle and sort phases occur simultaneously i.e. while outputs are being fetched they are merged.
    SecondarySort

    If equivalence rules for keys while grouping the intermediates are different from those for grouping keys before reduction, then one may specify a Comparator via JobConf.setOutputValueGroupingComparator(Class).Since JobConf.setOutputKeyComparatorClass(Class) can be used to control how intermediate keys are grouped, these can be used in conjunction to simulate secondary sort on values.
    For example, say that you want to find duplicate web pages and tag them all with the url of the "best" known example. You would set up the job like:
        Map Input Key: url
        Map Input Value: document
        Map Output Key: document checksum, url pagerank
        Map Output Value: url
        Partitioner: by checksum
        OutputKeyComparator: by checksum and then decreasing pagerank
        OutputValueGroupingComparator: by checksum
    Reduce

    In this phase the reduce(Object, Iterator, OutputCollector, Reporter) method is called for each <key, (list of values)> pair in the grouped inputs.

    The output of the reduce task is typically written to the FileSystem via OutputCollector.collect(Object, Object).

The output of the Reducer is not re-sorted.

Example:

         public class MyReducer<K extends WritableComparable, V extends Writable> 
         extends MapReduceBase implements Reducer<K, V, K, V> {
         
           static enum MyCounters { NUM_RECORDS }
            
           private String reduceTaskId;
           private int noKeys = 0;
           
           public void configure(JobConf job) {
             reduceTaskId = job.get(JobContext.TASK_ATTEMPT_ID);
           }
           
           public void reduce(K key, Iterator<V> values,
                              OutputCollector<K, V> output, 
                              Reporter reporter)
           throws IOException {
           
             // Process
             int noValues = 0;
             while (values.hasNext()) {
               V value = values.next();
               
               // Increment the no. of values for this key
               ++noValues;
               
               // Process the <key, value> pair (assume this takes a while)
               // ...
               // ...
               
               // Let the framework know that we are alive, and kicking!
               if ((noValues%10) == 0) {
                 reporter.progress();
               }
             
               // Process some more
               // ...
               // ...
               
               // Output the <key, value> 
               output.collect(key, value);
             }
             
             // Increment the no. of <key, list of values> pairs processed
             ++noKeys;
             
             // Increment counters
             reporter.incrCounter(NUM_RECORDS, 1);
             
             // Every 100 keys update application-level status
             if ((noKeys%100) == 0) {
               reporter.setStatus(reduceTaskId + " processed " + noKeys);
             }
           }
         }
    
7 JobConf

 A map/reduce job configuration.

JobConf is the primary interface for a user to describe a map-reduce job to the Hadoop framework for execution. The framework tries to faithfully execute the job as-is described by JobConf, however:

    Some configuration parameters might have been marked as final by administrators and hence cannot be altered.
    While some job parameters are straight-forward to set (e.g. setNumReduceTasks(int)), some parameters interact subtly rest of the framework and/or job-configuration and is relatively more complex for the user to control finely (e.g. setNumMapTasks(int)).

JobConf typically specifies the Mapper, combiner (if any), Partitioner, Reducer, InputFormat and OutputFormat implementations to be used etc.

Optionally JobConf is used to specify other advanced facets of the job such as Comparators to be used, files to be put in the DistributedCache, whether or not intermediate and/or job outputs are to be compressed (and how), debugability via user-provided scripts ( setMapDebugScript(String)/setReduceDebugScript(String)), for doing post-processing on task logs, task's stdout, stderr, syslog. and etc.

Here is an example on how to configure a job via JobConf:

         // Create a new JobConf
         JobConf job = new JobConf(new Configuration(), MyJob.class);
         
         // Specify various job-specific parameters     
         job.setJobName("myjob");
         
         FileInputFormat.setInputPaths(job, new Path("in"));
         FileOutputFormat.setOutputPath(job, new Path("out"));
         
         job.setMapperClass(MyJob.MyMapper.class);
         job.setCombinerClass(MyJob.MyReducer.class);
         job.setReducerClass(MyJob.MyReducer.class);
         
         job.setInputFormat(SequenceFileInputFormat.class);
         job.setOutputFormat(SequenceFileOutputFormat.class);
     
8. JobClient

 JobClient is the primary interface for the user-job to interact with the cluster. JobClient provides facilities to submit jobs, track their progress, access component-tasks' reports/logs, get the Map-Reduce cluster status information etc.

The job submission process involves:

    Checking the input and output specifications of the job.
    Computing the InputSplits for the job.
    Setup the requisite accounting information for the DistributedCache of the job, if necessary.
    Copying the job's jar and configuration to the map-reduce system directory on the distributed file-system.
    Submitting the job to the cluster and optionally monitoring it's status.

Normally the user creates the application, describes various facets of the job via JobConf and then uses the JobClient to submit the job and monitor its progress.

Here is an example on how to use JobClient:

         // Create a new JobConf
         JobConf job = new JobConf(new Configuration(), MyJob.class);
         
         // Specify various job-specific parameters     
         job.setJobName("myjob");
         
         job.setInputPath(new Path("in"));
         job.setOutputPath(new Path("out"));
         
         job.setMapperClass(MyJob.MyMapper.class);
         job.setReducerClass(MyJob.MyReducer.class);

         // Submit the job, then poll for progress until the job is complete
         JobClient.runJob(job);
    
Job Control

At times clients would chain map-reduce jobs to accomplish complex tasks which cannot be done via a single map-reduce job. This is fairly easy since the output of the job, typically, goes to distributed file-system and that can be used as the input for the next job.

However, this also means that the onus on ensuring jobs are complete (success/failure) lies squarely on the clients. In such situations the various job-control options are:

    runJob(JobConf) : submits the job and returns only after the job has completed.
    submitJob(JobConf) : only submits the job, then poll the returned handle to the RunningJob to query status and make scheduling decisions.
    JobConf.setJobEndNotificationURI(String) : setup a notification on job-completion, thus avoiding polling.
 

Part 2
http://www.infoq.com/cn/articles/bigdata-store-choose 
