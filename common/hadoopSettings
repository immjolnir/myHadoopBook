1. mapred.job.queue.name: name of the queue
	JobConf.java:
		getQueueName(); get("mapred.job.queue.name", DEFAULT_QUEUE_NAME);
		setQueueName(String queuename); set("mapred.job.queue.name", queueName);
2. mapred.reduce.tasks: reduce instance number, suffle
	
3. mapred.job.name: set the user-specified job name
	JobConf.java:
		setJobName(String name); or set("mapred.job.name", jobName);
4. In hadoop Streaming,
	stream.map.output.fild.separator: set it as the field separator for the map outputs
	stream.num.map.output.key.fields:  
	"stream.reduce.output.field.separator=SEP" and "stream.num.reduce.output.fiels=NUM" to specify the nth field separator in a line of the reduce outputs as the separator between the key and the value.

5. To generate gzip files as your generated ouptut, Pass 
"mapred.output.compress=true" and "mapred.output.compression.codec=org.apache.hadoop.io.compress.GzipCode" as options to your streaming job.

6. io.file.buffer.size
	hadoop always set it 4096 by default. org.apache.hadoop.fs.FileSystem.java
	  /**
	   * Opens an FSDataInputStream at the indicated Path.
	   * @param f the file to open
	   */
	  public FSDataInputStream open(Path f) throws IOException {
	    return open(f, getConf().getInt("io.file.buffer.size", 4096));
	  }
7. mapred.linerecordreader.maxlength: in LineRecordReader.java
public LineRecordReader( ... ) {
   thismaxLineLength = job.getInt("mapred.linerecordreader.maxlength",
                                    Integer.MAX_VALUE);
}

8. what is the JobContext? why only get* method is included?
Configuration configuration = context.getConfiguration();

And there is main methdod, 
  /** For debugging.  List non-default properties to the terminal and exit. */
  public static void main(String[] args) throws Exception {
    new Configuration().writeXml(System.out);
  }
in the Configuration.java.


9. how to configure?
Add the to the core-site.xml, to resolve that namenode is alwarys reformat when hadoop service is restarted.
	<property>
	  <name>hadoop.tmp.dir</name>
	  <value>${user.home}/hadoop-tmp.DO_NOT_DELETE</value>
	  <description>A base for other temporary directories.</description>
	</property>

$ hadoop namenode -format
	14/02/23 09:26:12 INFO namenode.NameNode: STARTUP_MSG: 
	/************************************************************
	STARTUP_MSG: Starting NameNode
	STARTUP_MSG:   host = fedora.heisenbug-20.mr.com/127.0.0.1
	STARTUP_MSG:   args = [-format]
	STARTUP_MSG:   version = 1.2.1
	STARTUP_MSG:   build = https://svn.apache.org/repos/asf/hadoop/common/branches/branch-1.2 -r 1503152; compiled by 'mattf' on Mon Jul 22 15:23:09 PDT 2013
	STARTUP_MSG:   java = 1.7.0_45
	************************************************************/
	14/02/23 09:26:12 INFO util.GSet: Computing capacity for map BlocksMap
	14/02/23 09:26:12 INFO util.GSet: VM type       = 32-bit
	14/02/23 09:26:12 INFO util.GSet: 2.0% max memory = 932184064
	14/02/23 09:26:12 INFO util.GSet: capacity      = 2^22 = 4194304 entries
	14/02/23 09:26:12 INFO util.GSet: recommended=4194304, actual=4194304
	14/02/23 09:26:13 INFO namenode.FSNamesystem: fsOwner=zhishan
	14/02/23 09:26:13 INFO namenode.FSNamesystem: supergroup=supergroup
	14/02/23 09:26:13 INFO namenode.FSNamesystem: isPermissionEnabled=true
	14/02/23 09:26:13 INFO namenode.FSNamesystem: dfs.block.invalidate.limit=100
	14/02/23 09:26:13 INFO namenode.FSNamesystem: isAccessTokenEnabled=false accessKeyUpdateInterval=0 min(s), accessTokenLifetime=0 min(s)
	14/02/23 09:26:13 INFO namenode.FSEditLog: dfs.namenode.edits.toleration.length = 0
	14/02/23 09:26:13 INFO namenode.NameNode: Caching file names occuring more than 10 times 
	14/02/23 09:26:13 INFO common.Storage: Image file /home/zhishan/hadoop-tmp.DO_NOT_DELETE/dfs/name/current/fsimage of size 113 bytes saved in 0 seconds.
	14/02/23 09:26:13 INFO namenode.FSEditLog: closing edit log: position=4, editlog=/home/zhishan/hadoop-tmp.DO_NOT_DELETE/dfs/name/current/edits
	14/02/23 09:26:13 INFO namenode.FSEditLog: close success: truncate to 4, editlog=/home/zhishan/hadoop-tmp.DO_NOT_DELETE/dfs/name/current/edits
	14/02/23 09:26:13 INFO common.Storage: Storage directory /home/zhishan/hadoop-tmp.DO_NOT_DELETE/dfs/name has been successfully formatted.
	14/02/23 09:26:13 INFO namenode.NameNode: SHUTDOWN_MSG: 
	/************************************************************
	SHUTDOWN_MSG: Shutting down NameNode at fedora.heisenbug-20.mr.com/127.0.0.1
	************************************************************/
Althogh, the variable ${user.home} could not got from the Configuration, but it can be retrieved from the Hadoop JVM instance as its property.


10.  org.apache.hadoop.conf.Configuration: map.input.file is deprecated. Instead, use mapreduce.map.input.file
conf.get("mapreduce.map.input.file", "in mapper job return the input file name");
