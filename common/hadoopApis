1. FileStatus, in ch03
	Interface that represents the client side information for a file.

2. FileUtil, org.apache.hadoop.fs
	A collection of file-processing util methods
	  public static Path[] stat2Paths(FileStatus[] stats): stats[i].getPath();
	/** Copy files between FileSystems. */
	  public static boolean copy(FileSystem srcFS, Path src, 
				     FileSystem dstFS, Path dst, 
				     boolean deleteSource,
				     Configuration conf)

3. Reporter API
http://hadoop.apache.org/docs/r2.2.0/api/org/apache/hadoop/mapred/Reporter.html#getInputSplit%28%29

InputSplit getInputSplit() throws UnsupportedOperationException

    Get the InputSplit object for a map.

    Returns:
        the InputSplit that the map is reading from.

e.g
public class InputSplitNameGainer {
    @SuppressWarnings({ "rawtypes", "unchecked" })
    public static String getInputSplitName(Reporter reporter) {

        String inputFile = "";
        try {
            Class clazz = Class.forName("org.apache.hadoop.mapred.lib.TaggedInputSplit");
            if (reporter.getInputSplit() instanceof FileSplit) {
                inputFile = ((FileSplit) reporter.getInputSplit()).getPath().toUri().toString();
            } else  if (clazz.isAssignableFrom(reporter.getInputSplit().getClass()))
            {
                Method method = clazz.getMethod("getInputSplit", new Class[0]);
                method.setAccessible(true);
                InputSplit iSplit =  (InputSplit) method.invoke(reporter.getInputSplit(), new Object[0]);
                inputFile = ((FileSplit) iSplit).getPath().toUri().toString();
            }
        } catch (Exception e) {
        	e.printStackTrace();
        }
        return inputFile;
    }
}

	string mMapFile = InputSplitNameGainer.getInputSplitName(reporter);
        logger.log(Level.INFO, where + "map.input.file = " + mMapFile);
        logger.log(Level.INFO, where + "mapred.task.id = " + conf.get("mapred.task.id"));

By get mMapFile's URL, we can get its path, and other files, if its path is solid respactive with the mMapFile's Path.

5. What is the Reducer?
http://hadoop.apache.org/docs/r2.2.0/api/org/apache/hadoop/mapreduce/Reducer.html

Ruduces a set of intermediate values which share a key to a smaller set of values.
Reducer implementations can access the Configuration for the job via the JobContext.getConfiguration() method.
Reducer has 3 primary phases:

1. Shuffle
  The Reducer copies the sorted output from each Mapper using HTTP across the network.

2. Sort
  The framework merge sorts Reducer inputs by keys(since different Mappers may have output the same key). 
  The shuffle and sort phases occur simultanceously, i.e while outputs are being fetched they are merged.
   SecondarySort
 To achieve a secondary sort on the values returned by the value iterator, the application should extend the key with the secondary key and define a grouping comparator. The keys will be sorted using the entire key, but will be grouped using the grouping comparator to decide which keys and values are sent in the same call to reduce.The grouping comparator is specified via Job.setGroupingComparatorClass(Class). The sort order is controlled by Job.setSortComparatorClass(Class).

For example, say that you want to find duplicate web pages and tag them all with the url of the "best" known example. You would set up the job like:
Map Input Key: url
Map Input Value: document
Map Output Key: document checksum, url pagerank
Map Output Value: url
Partitioner: by checksum
OutputKeyComparator: by checksum and then decreasing pagerank
OutputValueGroupingComparator: by checksum

3. Reduce
In this phase, the reduce(Object, Iterable, Context) method is called for each<key, (collection of values)> in the sorted inputs.
The output of the reduce task is typically written to a RecordWriter via TaskInputOutputContext.write(Object, Object).
The output ot the Reducer is not re-sorted.

Example:

 public class IntSumReducer<Key> extends Reducer<Key,IntWritable,
                                                 Key,IntWritable> {
   private IntWritable result = new IntWritable();
 
   public void reduce(Key key, Iterable<IntWritable> values,
                      Context context) throws IOException, InterruptedException {
     int sum = 0;
     for (IntWritable val : values) {
       sum += val.get();
     }
     result.set(sum);
     context.write(key, result);
   }
 }

